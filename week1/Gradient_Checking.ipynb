{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "Chào mừng bạn đến với bài tập cuối cùng cho tuần này! Trong bài tập này, bạn sẽ thực hiện kiểm tra độ dốc.\n",
    "\n",
    "Vào cuối sổ tay này, bạn sẽ có thể:\n",
    "\n",
    "Thực hiện kiểm tra độ dốc để xác minh tính chính xác của việc triển khai backprop của bạn\n",
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Problem Statement](#2)\n",
    "- [3 - How does Gradient Checking work?](#3)\n",
    "- [4 - 1-Dimensional Gradient Checking](#4)\n",
    "    - [Exercise 1 - forward_propagation](#ex-1)\n",
    "    - [Exercise 2 - backward_propagation](#ex-2)\n",
    "    - [Exercise 3 - gradient_check](#ex-3)\n",
    "- [5 - N-Dimensional Gradient Checking](#5)\n",
    "    - [Exercise 4 - gradient_check_n](#ex-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from testcase import *\n",
    "from public_tests import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Problem Statement\n",
    "Bạn là thành viên của nhóm làm việc để cung cấp thanh toán di động trên toàn cầu và được yêu cầu xây dựng mô hình học sâu để phát hiện gian lận - bất cứ khi nào ai đó thanh toán, bạn muốn xem liệu khoản thanh toán đó có gian lận hay không, chẳng hạn như nếu người dùng tài khoản đã bị tin tặc chiếm đoạt.\n",
    "\n",
    "Bạn đã biết rằng việc triển khai backpropagation khá khó khăn và đôi khi có lỗi. Bởi vì đây là một ứng dụng quan trọng, nên Giám đốc điều hành của công ty bạn muốn thực sự chắc chắn rằng việc triển khai nhân giống ngược của bạn là đúng. Giám đốc điều hành của bạn nói, \"Hãy cho tôi bằng chứng rằng sự nhân rộng của bạn đang thực sự hoạt động!\" Để đảm bảo điều này, bạn sẽ sử dụng \"kiểm tra độ dốc\".\n",
    "\n",
    "Hãy làm nó!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - How does Gradient Checking work?\n",
    "Backpropagation tính toán các độ dốc $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ biểu thị các tham số của mô hình. $ J $ được tính bằng cách sử dụng chức năng lan truyền chuyển tiếp và hàm mất mát của bạn.\n",
    "\n",
    "Bởi vì truyền tiến tương đối dễ thực hiện, bạn tự tin rằng mình đã hiểu đúng và vì vậy bạn gần như chắc chắn 100% rằng bạn đang tính đúng chi phí $ J $. Do đó, bạn có thể sử dụng mã tính toán $ J $ của mình để xác minh mã tính toán $\\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Hãy xem lại định nghĩa của đạo hàm (hoặc gradient): $$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} $$\n",
    "\n",
    "Nếu bạn không quen thuộc với ký hiệu \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\", thì đó chỉ là một cách nói \"when $\\varepsilon$ thực sự rất nhỏ.\"\n",
    "\n",
    "Bạn biết những điều sau:\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\theta} $ là những gì bạn muốn để đảm bảo rằng bạn đang tính toán chính xác.\n",
    "Bạn có thể tính $J(\\theta + \\varepsilon)$ và  $J(\\theta - \\varepsilon)$ (trong trường hợp  $\\theta$ là số thực), vì bạn tự tin triển khai với $ J $ đúng.\n",
    "Hãy sử dụng phương trình (1) và một giá trị nhỏ cho $\\varepsilon$ để thuyết phục Giám đốc điều hành của bạn rằng mã tính toán $\\frac{\\partial J}{\\partial \\theta}$ của bạn là đúng!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a name='4'></a>\n",
    "## 4 - 1-Dimensional Gradient Checking\n",
    "Xét một hàm tuyến tính 1D $J(\\theta) = \\theta x$. Mô hình chỉ chứa một tham số có giá trị thực duy nhất $\\theta$ và lấy $x$ làm đầu vào.\n",
    "\n",
    "Bạn sẽ triển khai mã để tính $ J (.) $ Và dẫn xuất của nó $\\frac{\\partial J}{\\partial \\theta}$. Sau đó, bạn sẽ sử dụng kiểm tra độ dốc để đảm bảo rằng tính toán phái sinh của bạn cho $ J $ là chính xác.\n",
    "\n",
    "<img src = \"images/1Dgrad_kiank.png\" style = \"width: 600px; height: 250px;\">\n",
    "<caption> <center> <font color = 'Purple'> <b> Hình 1 </b>: Mô hình tuyến tính 1D </font></center> </caption>\n",
    "\n",
    "Sơ đồ trên cho thấy các bước tính toán chính: Đầu tiên bắt đầu với $ x $, sau đó đánh giá hàm $ J (x) $ (\"truyền về phía trước\"). Sau đó, tính đạo hàm $\\frac{\\partial J}{\\partial \\theta}$ (\"lan truyền ngược\").\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - forward_propagation\n",
    "\n",
    "Thực hiện`forward propagation`. Đối với hàm đơn giản này, hãy tính $ J (.) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    \n",
    "    # (approx. 1 line)\n",
    "    J = theta * x\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))\n",
    "forward_propagation_test(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ, hãy thực hiện bước `truyền ngược` (tính toán đạo hàm) của Hình 1. Tức là, tính đạo hàm của $J(\\theta) = \\theta x$ đối với $ \\theta $. Để giúp bạn không phải làm phép tính, bạn sẽ nhận được $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the derivative of J with respect to theta (see Figure 1).\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- the gradient of the cost with respect to theta\n",
    "    \"\"\"\n",
    "    \n",
    "    # (approx. 1 line)\n",
    "    dtheta = x\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))\n",
    "backward_propagation_test(backward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'> </a>\n",
    "### Bài tập 3 - gradient_check\n",
    "\n",
    "Để cho thấy rằng hàm `back_propagation ()` đang tính toán chính xác độ dốc $ \\ frac {\\ một phần J} {\\ part \\ theta} $, hãy triển khai kiểm tra độ dốc.\n",
    "\n",
    "**Hướng dẫn**:\n",
    "- Đầu tiên tính toán \"gradapprox\" bằng công thức trên (1) và một giá trị nhỏ là $ \\ varepsilon $. Dưới đây là các bước để làm theo:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Sau đó, tính toán gradient bằng cách sử dụng lan truyền ngược và lưu trữ kết quả trong một biến \"grad\"\n",
    "- Cuối cùng, tính toán sự khác biệt tương đối giữa \"gradapprox\" và \"grad\" bằng công thức sau:\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "Bạn sẽ cần 3 Bước để tính công thức này:\n",
    "   - 1 '. tính toán tử số bằng np.linalg.norm (...)\n",
    "   - 2 '. tính mẫu số. Bạn sẽ cần gọi np.linalg.norm (...) hai lần.\n",
    "   - 3 '. chia chúng.\n",
    "- Nếu sự khác biệt này là nhỏ (nhỏ hơn $ 10 ^ {- 7} $), bạn có thể hoàn toàn tự tin rằng bạn đã tính toán gradient của mình một cách chính xác. Nếu không, có thể xảy ra sai sót trong tính toán gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check\n",
    "\n",
    "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in Figure 1.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a float input\n",
    "    theta -- our parameter, a float as well\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
    "    # (approx. 5 lines)\n",
    "    theta_plus = theta + epsilon                               # Step 1\n",
    "    theta_minus =   theta - epsilon                              # Step 2\n",
    "    J_plus =  forward_propagation(x,theta_plus)                                # Step 3\n",
    "    J_minus =    forward_propagation(x,theta_minus)                              # Step 4\n",
    "    gradapprox =  (J_plus-J_minus) /(2*epsilon)                             # Step 5\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
    "    #(approx. 1 line)\n",
    "    grad =backward_propagation(x, theta)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    #(approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad-gradapprox)                                # Step 1'\n",
    "    denominator =  np.linalg.norm(grad)+np.linalg.norm(gradapprox)                             # Step 2'\n",
    "    difference = numerator / denominator                               # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.919335883291695e-10\u001b[0m\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(2,4, print_msg=True)\n",
    "\n",
    "gradient_check_test(gradient_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xin chúc mừng, sự khác biệt nhỏ hơn ngưỡng $ 10 ^ {- 7} $. Vì vậy, bạn có thể tin tưởng cao rằng bạn đã tính toán chính xác gradient trong `back_propagation () '.\n",
    "\n",
    "Bây giờ, trong trường hợp tổng quát hơn, hàm chi phí $ J $ của bạn có nhiều hơn một đầu vào 1D duy nhất. Khi bạn đang huấn luyện một mạng nơron, $ \\ theta $ thực sự bao gồm nhiều ma trận $ W ^ {[l]} $ và bias $ b ^ {[l]} $! Điều quan trọng là phải biết cách thực hiện kiểm tra độ dốc với các đầu vào có chiều cao hơn. Hãy làm nó!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - N-Dimensional Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình sau đây mô tả quá trình lan truyền tiến và lùi của mô hình phát hiện gian lận của bạn.\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: Deep neural network. LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID</font></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy xem các cách triển khai của bạn để biết cách tuyên truyền tiến và truyền ngược."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples \n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (5, 4)\n",
    "                    b1 -- bias vector of shape (5, 1)\n",
    "                    W2 -- weight matrix of shape (3, 5)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for one example)\n",
    "    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(log_probs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn đã nhận được một số kết quả trong bộ kiểm tra phát hiện gian lận nhưng bạn không chắc chắn 100% về mô hình của mình. Không ai là hoàn hảo! Hãy triển khai kiểm tra gradient để xác minh xem gradient của bạn có chính xác hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kiểm tra độ dốc hoạt động như thế nào?**.\n",
    "\n",
    "Như trong Phần 3 và 4, bạn muốn so sánh \"gradapprox\" với gradient được tính bằng cách cộng dồn ngược. Công thức vẫn là:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Tuy nhiên, $ \\theta $ không còn là một đại lượng vô hướng nữa. Nó là một từ điển được gọi là \"tham số\". Hàm \"` dictionary_to_vector () `\" đã được triển khai cho bạn. Nó chuyển đổi từ điển \"tham số\" thành một vectơ được gọi là \"giá trị\", thu được bằng cách định hình lại tất cả các tham số (W1, b1, W2, b2, W3, b3) thành vectơ và nối chúng.\n",
    "\n",
    "Hàm nghịch đảo là \"` vector_to_dictionary` \"sẽ trả về từ điển\" tham số \".\n",
    "\n",
    "<img src = \"images/dictionary_to_vector.png\" style = \"width: 600px; height: 400px;\">\n",
    "<caption> <center> <font color = 'Purple'> <b> Hình 2 </b>: dictionary_to_vector () và vector_to_dictionary (). Bạn sẽ cần các hàm này trong gradient_check_n () </font> </center> </caption>\n",
    "\n",
    "Từ điển \"gradient\" cũng đã được chuyển đổi thành vector \"grad\" bằng cách sử dụng gradient_to_vector (), vì vậy bạn không cần phải lo lắng về điều đó.\n",
    "\n",
    "Bây giờ, đối với mỗi tham số trong vectơ của bạn, bạn sẽ áp dụng quy trình tương tự như đối với bài tập gradient_check. Bạn sẽ lưu trữ mỗi xấp xỉ gradient trong một vectơ `grad_approx`. Nếu việc kiểm tra diễn ra như mong đợi, mỗi giá trị trong giá trị gần đúng này phải khớp với các giá trị gradient thực được lưu trữ trong vectơ `grad`.\n",
    "\n",
    "Lưu ý rằng `grad` được tính bằng cách sử dụng hàm` gradient_to_vector`, sử dụng các đầu ra gradient của hàm `back_propagation_n`.\n",
    "\n",
    "<a name='ex-4'> </a>\n",
    "### Bài tập 4 - gradient_check_n\n",
    "\n",
    "Thực hiện chức năng bên dưới.\n",
    "\n",
    "**Hướng dẫn**: Đây là mã giả sẽ giúp bạn thực hiện kiểm tra độ dốc.\n",
    "\n",
    "Đối với mỗi tôi trong num_parameters:\n",
    "- Để tính toán `J_plus [i]`:\n",
    "    1. Đặt $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Đặt $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Tính $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))\n",
    "- Để tính `J_minus [i]`: làm tương tự với $ \\theta ^ {-} $\n",
    "- Tính $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Do đó, bạn nhận được một gradapprox vectơ, trong đó gradapprox [i] là một xấp xỉ của gradient đối với `tham_số_giá_trị [i]`. Bây giờ bạn có thể so sánh vectơ gradapprox này với vectơ gradient từ sự lan truyền ngược. Cũng giống như trường hợp 1D (Bước 1 ', 2', 3 '), hãy tính:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "**Lưu ý**: Sử dụng `np.linalg.norm` để lấy định mức"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    grad_approx = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        #(approx. 3 lines)\n",
    "        theta_plus = np.copy(parameters_values)                                       # Step 1\n",
    "        theta_plus[i]=  theta_plus[i] +epsilon                                   # Step 2\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))                                    # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        #(approx. 3 lines)\n",
    "        theta_minus = np.copy(parameters_values)                                   # Step 1\n",
    "        theta_minus[i] =  theta_minus[i] - epsilon                                # Step 2        \n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))                                # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        # (approx. 1 line)\n",
    "        grad_approx[i] = (J_plus[i]-J_minus[i])/(2*epsilon)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    # (approx. 1 line)\n",
    "    numerator =   np.linalg.norm(grad-grad_approx)                                          # Step 1'\n",
    "    denominator =    np.linalg.norm(grad)+np.linalg.norm(grad_approx)                                       # Step 2'\n",
    "    difference =     numerator / denominator                                       # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThere is a mistake in the backward propagation! difference = 0.28509315678069896\u001b[0m\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)\n",
    "assert not(type(difference) == np.ndarray), \"You are not using np.linalg.norm for numerator or denominator\"\n",
    "\n",
    "gradient_check_n_test(gradient_check_n, parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghi chú**\n",
    "- Kiểm tra Gradient chậm! Tính gần đúng độ dốc với $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ rất tốn kém về mặt tính toán . Vì lý do này, chúng tôi không chạy kiểm tra độ dốc ở mỗi lần lặp trong quá trình đào tạo. Chỉ cần một vài lần để kiểm tra xem gradient có chính xác hay không.\n",
    "- Kiểm tra Gradient, ít nhất là như chúng tôi đã trình bày, không hoạt động với tình trạng bỏ học. Bạn thường sẽ chạy thuật toán kiểm tra độ dốc mà không bị bỏ rơi để đảm bảo rằng backprop của bạn là chính xác, sau đó thêm droppout.\n",
    "\n",
    "Chúc mừng! Giờ đây, bạn có thể tự tin rằng mô hình học sâu để phát hiện gian lận của mình đang hoạt động chính xác! Bạn thậm chí có thể sử dụng điều này để thuyết phục Giám đốc điều hành của mình. :)\n",
    "<br>\n",
    "<font color = 'blue'>\n",
    "    \n",
    "Những gì bạn nên nhớ từ sổ tay này :\n",
    "- Kiểm tra độ dốc xác minh sự gần gũi giữa các độ dốc từ lan truyền ngược và xấp xỉ số của độ dốc (được tính bằng cách sử dụng truyền tiến).\n",
    "- Kiểm tra độ dốc chậm, vì vậy bạn không muốn chạy nó trong mỗi lần luyện tập lặp lại. Bạn thường chỉ chạy nó để đảm bảo mã của bạn là chính xác, sau đó tắt nó đi và sử dụng backprop cho quá trình học thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
